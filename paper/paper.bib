@inbook{pytorch,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12}
}

@software{nflows,
  author       = {Conor Durkan and
                  Artur Bekasov and
                  Iain Murray and
                  George Papamakarios},
  title        = {{nflows}: normalizing flows in {PyTorch}},
  month        = nov,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.14},
  doi          = {10.5281/zenodo.4296287},
  url          = {https://doi.org/10.5281/zenodo.4296287}
}

@article{normflows, 
  author = {Vincent Stimper and David Liu and Andrew Campbell and Vincent Berenz and Lukas Ryll and Bernhard Schölkopf and José Miguel Hernández-Lobato}, 
  title = {normflows: A PyTorch Package for Normalizing Flows}, 
  journal = {Journal of Open Source Software}, 
  volume = {8},
  number = {86}, 
  pages = {5361}, 
  publisher = {The Open Journal}, 
  doi = {10.21105/joss.05361}, 
  url = {https://doi.org/10.21105/joss.05361}, 
  year = {2023}
} 

@inproceedings{gradabm,
  title = {Differentiable Agent-Based Epidemiology},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  author = {Chopra, Ayush and Rodr{\'i}guez, Alexander and Subramanian, Jayakumar and {Quera-Bofarull}, Arnau and Krishnamurthy, Balaji and Prakash, B. Aditya and Raskar, Ramesh},
  year = {2023},
  series = {{{AAMAS}} '23},
  pages = {1848--1857},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  address = {{Richland, SC}},
  abstract = {Mechanistic simulators are an indispensable tool for epidemiology to explore the behavior of complex, dynamic infections under varying conditions and navigate uncertain environments. Agent-based models (ABMs) are an increasingly popular simulation paradigm that can represent the heterogeneity of contact interactions with granular detail and agency of individual behavior. However, conventional ABM frameworks not differentiable and present challenges in scalability; due to which it is non-trivial to connect them to auxiliary data sources. In this paper, we introduce GradABM: a scalable, differentiable design for agent-based modeling that is amenable to gradient-based learning with automatic differentiation. GradABM can quickly simulate million-size populations in few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous data sources. This provides an array of practical benefits for calibration, forecasting, and evaluating policy interventions. We demonstrate the efficacy of GradABM via extensive experiments with real COVID-19 and influenza datasets.},
  isbn = {978-1-4503-9432-1},
  keywords = {automatic differentiation,computational epidemiology,deep neural networks,differentiable agent-based modeling}
}

@article{hep,
  title={Differentiable programming in high-energy physics},
  author={Baydin, At{\i}l{\i}m G{\"u}nes and NYU, Kyle Cranmer and Feickert, Matthew and Gray, Lindsey and Heinrich, Lukas and NYU, Alexander Held and Neubauer, Andrew Melo Vanderbilt Mark and Pearkes, Jannicke and Simpson, Nathan and Smith, Nick and others},
  year={2020}
}

@article{ii,
  title={Indirect inference},
  author={Gourieroux, Christian and Monfort, Alain and Renault, Eric},
  journal={Journal of applied econometrics},
  volume={8},
  number={S1},
  pages={S85--S118},
  year={1993},
  publisher={Wiley Online Library}
}

@article{smm,
  title={Applying the method of simulated moments to estimate a small agent-based asset pricing model},
  author={Franke, Reiner},
  journal={Journal of Empirical Finance},
  volume={16},
  number={5},
  pages={804--815},
  year={2009},
  doi={10.1016/j.jempfin.2009.06.006},
  publisher={Elsevier}
}

@article{mala,
  title={Exponential convergence of {L}angevin distributions and their discrete approximations},
  author={Roberts, Gareth O and Tweedie, Richard L},
  journal={Bernoulli},
  pages={341--363},
  year={1996},
  doi={10.2307/3318418},
  publisher={JSTOR}
}

@article{hmc,
  title={Hybrid {M}onte {C}arlo},
  author={Duane, Simon and Kennedy, Anthony D and Pendleton, Brian J and Roweth, Duncan},
  journal={Physics letters B},
  volume={195},
  number={2},
  pages={216--222},
  year={1987},
  publisher={Elsevier}
}

@article{bissiri,
  title={A general framework for updating belief distributions},
  author={Bissiri, Pier Giovanni and Holmes, Chris and Walker, Stephen G},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={78},
  number={5},
  pages={1103},
  year={2016},
  doi={10.1111/rssb.12158},
  publisher={Wiley-Blackwell}
}

@article{gvi,
  title={{An optimization-centric view on {B}ayes’ rule: Reviewing and generalizing variational inference}},
  author={Knoblauch, Jeremias and Jewson, Jack and Damoulas, Theodoros},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={132},
  pages={1--109},
  year={2022}
}

@article{ai4abm,
  title={{B}ayesian calibration of differentiable agent-based models},
  author={Quera-Bofarull, Arnau and Chopra, Ayush and Calinescu, Anisoara and Wooldridge, Michael and Dyer, Joel},
  journal={ICLR Workshop on AI for Agent-based Modelling},
  year={2023}
}

@article{dae,
  title={Some challenges of calibrating differentiable agent-based models},
  author={Quera-Bofarull, Arnau and Dyer, Joel and Calinescu, Anisoara and Wooldridge, Michael},
  journal={ICML Differentiable Almost Everything Workshop},
  year={2023}
}

@article{sbi, 
  doi = {10.21105/joss.02505}, 
  url = {https://doi.org/10.21105/joss.02505}, 
  year = {2020}, 
  publisher = {The Open Journal}, 
  volume = {5}, 
  number = {52}, 
  pages = {2505}, 
  author = {Alvaro Tejero-Cantero and Jan Boelts and Michael Deistler and Jan-Matthis Lueckmann and Conor Durkan and Pedro J. Gonçalves and David S. Greenberg and Jakob H. Macke}, 
  title = {sbi: A toolkit for simulation-based inference}, 
  journal = {Journal of Open Source Software} 
}

@article{blit, doi = {10.21105/joss.04622}, url = {https://doi.org/10.21105/joss.04622}, year = {2022}, publisher = {The Open Journal}, volume = {7}, number = {79}, pages = {4622}, author = {Marco Benedetti and Gennaro Catapano and Francesco De Sclavis and Marco Favorito and Aldo Glielmo and Davide Magnanimi and Antonio Muci}, title = {Black-it: A Ready-to-Use and Easy-to-Extend Calibration Kit for Agent-based Models}, journal = {Journal of Open Source Software} }

@article{pyvbmc, 
    doi = {10.21105/joss.05428}, 
    url = {https://doi.org/10.21105/joss.05428}, 
    year = {2023}, 
    publisher = {The Open Journal}, 
    volume = {8}, number = {86}, pages = {5428}, 
    author = {Bobby Huggins and Chengkun Li and Marlon Tobaben and Mikko J. Aarnos and Luigi Acerbi}, 
    title = {PyVBMC: Efficient {B}ayesian inference in Python}, journal = {Journal of Open Source Software}
}

@article{abcpy,
 title={{ABCpy}: A High-Performance Computing Perspective to Approximate {B}ayesian Computation},
 volume={100},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v100i07},
 doi={10.18637/jss.v100.i07},
 number={7},
 journal={Journal of Statistical Software},
 author={Dutta, Ritabrata and Schoengens, Marcel and Pacchiardi, Lorenzo and Ummadisingu, Avinash and Widmer, Nicole and K\"unzli, Pierre and Onnela, Jukka-Pekka and Mira, Antonietta},
 year={2021},
 pages={1–38}
}

@article{pyabc,
  title = {pyABC: Efficient and robust easy-to-use approximate {B}ayesian computation},
  author = {Schälte, Yannik and Klinger, Emmanuel and Alamoudi, Emad and Hasenauer, Jan},
  journal = {Journal of Open Source Software},
  publisher = {The Open Journal},
  year = {2022},
  volume = {7},
  number = {74},
  pages = {4304},
  doi = {10.21105/joss.04304},
  url = {https://doi.org/10.21105/joss.04304},
}

@misc{radev2023bayesflow,
  title = {BayesFlow: Amortized Bayesian Workflows With Neural Networks},
  author = {Stefan T Radev and Marvin Schmitt and Lukas Schumacher and Lasse Elsem\"{u}ller and Valentin Pratz and Yannik Sch\"{a}lte and Ullrich K\"{o}the and Paul-Christian B\"{u}rkner},
  year = {2023},
  publisher= {arXiv},
  url={https://arxiv.org/abs/2306.16015}
}